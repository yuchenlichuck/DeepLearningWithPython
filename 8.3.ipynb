{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "esides Deep Dream, another major development in deep learning-driven image modification that happened in the summer of 2015 is neural style transfer, introduced by Leon Gatys et al. The neural style transfer algorithm has undergone many refinements and spawned many variations since its original introduction, including a viral smartphone app, called Prisma. For simplicity, this section focuses on the formulation described in the original paper.\n",
    "\n",
    "Neural style transfer consists in applying the \"style\" of a reference image to a target image, while conserving the \"content\" of the target image:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'''loss = distance(style(reference_image) - style(generated_image)) +\n",
    "       distance(content(original_image) - content(generated_image))\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where distance is a norm function such as the L2 norm, content is a function that takes an image and computes a representation of its \"content\", and style is a function that takes an image and computes a representation of its \"style\".\n",
    "\n",
    "Minimizing this loss would cause style(generated_image) to be close to style(reference_image), while content(generated_image) would be close to content(generated_image), thus achieving style transfer as we defined it.\n",
    "\n",
    "A fundamental observation made by Gatys et al is that deep convolutional neural networks offer precisely a way to mathematically defined the style and content functions. Let's see how."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img,img_to_array\n",
    "\n",
    "target_image_path='E:/photo/55818884_3526418127582719_3897029842608434465_n.jpg'\n",
    "\n",
    "style_reference_image='E:/photo/mosaic.jpg'\n",
    "\n",
    "w,h=load_img(target_image_path).size\n",
    "img_h=400\n",
    "img_w=int(w*img_h/h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.applications import vgg19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = load_img(image_path, target_size=(img_h, img_w))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = vgg19.preprocess_input(img)\n",
    "    return img\n",
    "\n",
    "def deprocess_image(x):\n",
    "    # Remove zero-center by mean pixel\n",
    "    x[:, :, 0] += 103.939\n",
    "    x[:, :, 1] += 116.779\n",
    "    x[:, :, 2] += 123.68\n",
    "    # 'BGR'->'RGB'\n",
    "    x = x[:, :, ::-1]\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "target_image=K.constant(preprocess_image(target_image_path))\n",
    "style_reference_image=K.constant((preprocess_image(style_reference_image)))\n",
    "combination_image=K.placeholder((1,img_h,img_w,3))\n",
    "input_tensor =K.concatenate([target_image,style_reference_image,combination_image],axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
      "80142336/80134624 [==============================] - 151s 2us/step\n",
      "Model loaded\n"
     ]
    }
   ],
   "source": [
    "model = vgg19.VGG19(input_tensor=input_tensor,weights='imagenet',include_top=False)\n",
    "print('Model loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def content_loss(bass,combination):\n",
    "    return K.sum=(K.square(combination-bass))\n",
    "def gram_matrix(x):\n",
    "    features=K.batch_flatten(K.permute_dimensions(x,(2,0,1)))\n",
    "    gram=K.dot(features,K.transpose(features))\n",
    "    \n",
    "    return gram\n",
    "\n",
    "def style_loss(style,combination):\n",
    "    S=gram_matrix(style)\n",
    "    C=gram_matrix(combination)\n",
    "    size=img_h*img_w\n",
    "    return K.sum(K.square(\n",
    "                \n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
