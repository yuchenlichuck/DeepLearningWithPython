{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variational autoencoders\n",
    "Variational autoencoders, simultaneously discovered by Kingma & Welling in December 2013, and Rezende, Mohamed & Wierstra in January 2014, are a kind of generative model that is especially appropriate for the task of image editing via concept vectors. They are a modern take on autoencoders -- a type of network that aims to \"encode\" an input to a low-dimensional latent space then \"decode\" it back -- that mixes ideas from deep learning with Bayesian inference.\n",
    "\n",
    "A classical image autoencoder takes an image, maps it to a latent vector space via an \"encoder\" module, then decode it back to an output with the same dimensions as the original image, via a \"decoder\" module. It is then trained by using as target data the same images as the input images, meaning that the autoencoder learns to reconstruct the original inputs. By imposing various constraints on the \"code\", i.e. the output of the encoder, one can get the autoencoder to learn more or less interesting latent representations of the data. Most commonly, one would constraint the code to be very low-dimensional and sparse (i.e. mostly zeros), in which case the encoder acts as a way to compress the input data into fewer bits of information.\n",
    "\n",
    "Autoencoder\n",
    "\n",
    "In practice, such classical autoencoders don't lead to particularly useful or well-structured latent spaces. They're not particularly good at compression, either. For these reasons, they have largely fallen out of fashion over the past years. Variational autoencoders, however, augment autoencoders with a little bit of statistical magic that forces them to learn continuous, highly structured latent spaces. They have turned out to be a very powerful tool for image generation.\n",
    "\n",
    "A VAE, instead of compressing its input image into a fixed \"code\" in the latent space, turns the image into the parameters of a statistical distribution: a mean and a variance. Essentially, this means that we are assuming that the input image has been generated by a statistical process, and that the randomness of this process should be taken into accounting during encoding and decoding. The VAE then uses the mean and variance parameters to randomly sample one element of the distribution, and decodes that element back to the original input. The stochasticity of this process improves robustness and forces the latent space to encode meaningful representations everywhere, i.e. every point sampled in the latent will be decoded to a valid output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "\n",
    "img_shape=(28,28,1)\n",
    "batch_size=16\n",
    "latent_dim=2\n",
    "\n",
    "input_img=keras.Input(shape=img_shape)\n",
    "\n",
    "x=layers.Conv2D(32,3,\n",
    "               padding='same',activation='relu')(input_img)\n",
    "x=layers.Conv2D(64,3,\n",
    "               padding='same',activation='relu',\n",
    "               strides=(2,2))(x)\n",
    "x=layers.Conv2D(64,3,\n",
    "               padding='same',activation='relu')(x)\n",
    "x=layers.Conv2D(64,3,\n",
    "               padding='same',activation='relu')(x)\n",
    "\n",
    "\n",
    "shape_before_flattening=K.int_shape(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=layers.Flatten()(x)\n",
    "\n",
    "x=layers.Dense(32,activation='relu')(x)\n",
    "\n",
    "z_mean=layers.Dense(latent_dim)(x)\n",
    "z_log_var=layers.Dense(latent_dim)(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean,z_log_var=args\n",
    "    epsilon=K.random_normal(shape=(K.shape(z_mean)[0],latent_dim),\n",
    "                            mean=0.,stddev=1.)\n",
    "    return z_mean+K.exp(z_log_var)*epsilon\n",
    "\n",
    "z=layers.Lambda(sampling)([z_mean,z_log_var])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_input=layers.Input(K.int_shape(z)[1:])\n",
    "x=layers.Dense(np.prod(shape_before_flattening[1:]),\n",
    "              activation='relu')(decoder_input)\n",
    "\n",
    "x=layers.Reshape(shape_before_flattening[1:])(x)\n",
    "\n",
    "x=layers.Convolution2DTranspose(32,3,padding='same',activation='relu',strides=(2,2))(x)\n",
    "\n",
    "x=layers.Conv2D(1,3,padding='same',activation='sigmoid')(x)\n",
    "\n",
    "decoder=Model(decoder_input,x)\n",
    "z_decoded=decoder(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomVariationalLayer(keras.layers.Layer):\n",
    "    \n",
    "    def vae_loss(self,x,z_decoded):\n",
    "        x=K.flatten(x)\n",
    "        z_decoded=K.flatten(z_decoded)\n",
    "        xent_loss=keras.metrics.binary_crossentropy(x,z_decoded)\n",
    "        kl_loss=-5e-4 * K.mean(\n",
    "        1+z_log_var-K.square(z_mean)-K.exp(z_log_var),axis=-1)\n",
    "        return K.mean(xent_loss+kl_loss)\n",
    "    \n",
    "    def call(self,inputs):\n",
    "        x=inputs[0]\n",
    "        z_decoded=inputs[1]\n",
    "        loss=self.vae_loss(x,z_decoded)\n",
    "        self.add_loss(loss,inputs=inputs)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "    \n",
    "y=CustomVariationalLayer()([input_img,z_decoded])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "vae=Model(input_img,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae.compile(optimizer='rmsprop',loss=None)\n",
    "vae.summary()\n",
    "\n",
    "(x_train,_),(x_test,y_test)=mnist.load_data()\n",
    "x_train=x_train.astype('float32')/255.\n",
    "x_train=x_train.reshape(x_train.shape+(1,))\n",
    "x_test=x_test.astype('float32')/255.\n",
    "x_test="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
